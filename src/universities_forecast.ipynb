{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e6803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "import statsmodels.api as sm\n",
    "\n",
    "warnings.simplefilter(\"ignore\", ConvergenceWarning)\n",
    "\n",
    "# =============================\n",
    "# Configura√ß√µes\n",
    "# =============================\n",
    "STATIONS_CSV = \"../data/all_stations.csv\"\n",
    "UNI_CSV = \"./universidades_federais_coords_26_estados.csv\"\n",
    "CLEANED_BASE = \"../data/cleaned_data\"\n",
    "YEARS = [2019, 2020, 2021, 2022, 2023, 2024]\n",
    "RADIUS_KM = 35.0\n",
    "\n",
    "FORECAST_PARAMETER = \"TEMPERATURA DO AR - BULBO SECO, HORARIA (¬∞C)\"\n",
    "EXOG_VARS = [\n",
    "    \"RADIACAO GLOBAL (KJ/m¬≤)\",\n",
    "    \"PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB)\",\n",
    "    \"UMIDADE RELATIVA DO AR, HORARIA (%)\",\n",
    "]\n",
    "\n",
    "RESAMPLE_RULE = \"D\"     # m√©dia di√°ria\n",
    "SPLIT_TRAIN = 0.80      # 80/20 treino/teste\n",
    "ARIMA_ORDER = (3, 0, 1) # ARIMA(3,0,1)\n",
    "FOURIER_PERIOD = 365    # dias\n",
    "FOURIER_K = 4           # n¬∫ de harm√¥nicos (gera 2*K colunas)\n",
    "\n",
    "# =============================\n",
    "# Utilidades b√°sicas\n",
    "# =============================\n",
    "def calculate_distance_from_point_to_station(row, given_point_coord):\n",
    "    station_coord = (row[\"LATITUDE:\"], row[\"LONGITUDE:\"])\n",
    "    return geodesic(station_coord, given_point_coord).kilometers\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    y_true, y_pred = pd.Series(y_true), pd.Series(y_pred)\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true)))\n",
    "\n",
    "def create_fourier_terms(t, period, num_terms):\n",
    "    terms = []\n",
    "    for i in range(1, num_terms + 1):\n",
    "        terms.append(np.sin(2 * np.pi * i * t / period))\n",
    "        terms.append(np.cos(2 * np.pi * i * t / period))\n",
    "    return np.column_stack(terms)\n",
    "\n",
    "# =============================\n",
    "# Leitura de esta√ß√µes e dados\n",
    "# =============================\n",
    "def load_all_stations() -> pd.DataFrame:\n",
    "    df_all_stations = pd.read_csv(STATIONS_CSV, decimal=\",\", sep=\";\")\n",
    "    df_all_stations.columns = [c.strip() for c in df_all_stations.columns]\n",
    "    return df_all_stations\n",
    "\n",
    "def load_weather_for_station_filename(filename_2019: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Usa o nome-base de 2019 e monta os caminhos 2019‚Äì2024.\n",
    "    Concatena o que existir, e corrige 'RADIACAO GLOBAL (Kj/m¬≤)' -> 'RADIACAO GLOBAL (KJ/m¬≤)'.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for year in YEARS:\n",
    "        year_dir = f\"{year}_cleaned\"\n",
    "        target = filename_2019.replace(\"2019\", str(year))\n",
    "        path = os.path.join(CLEANED_BASE, year_dir, target)\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                df_weather_data = pd.read_csv(path, decimal=\".\", sep=\";\")\n",
    "                # üîß corre√ß√£o de nomenclatura (Kj ‚Üí KJ)\n",
    "                if 'RADIACAO GLOBAL (Kj/m¬≤)' in df_weather_data.columns:\n",
    "                    df_weather_data.rename(\n",
    "                        columns={'RADIACAO GLOBAL (Kj/m¬≤)': 'RADIACAO GLOBAL (KJ/m¬≤)'},\n",
    "                        inplace=True\n",
    "                    )\n",
    "                dfs.append(df_weather_data)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Falha ao ler {path}: {e}\")\n",
    "    if not dfs:\n",
    "        return None\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def build_daily_panel_from_stations(df_nearest_stations: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Retorna um DataFrame di√°rio (index=Data) com m√©dia das vari√°veis num√©ricas\n",
    "    das esta√ß√µes do raio. Mant√©m target + ex√≥genas, faz ffill().\n",
    "    \"\"\"\n",
    "    collected = []\n",
    "    for filename in df_nearest_stations[\"Arquivo\"]:\n",
    "        dfw = load_weather_for_station_filename(filename)\n",
    "        if dfw is not None:\n",
    "            collected.append(dfw)\n",
    "    if not collected:\n",
    "        return None\n",
    "\n",
    "    df = pd.concat(collected, ignore_index=True)\n",
    "    if \"Hora UTC\" in df.columns:\n",
    "        df = df.drop(columns=[\"Hora UTC\"])\n",
    "    df[\"Data\"] = pd.to_datetime(df[\"Data\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Data\"]).sort_values(\"Data\").set_index(\"Data\")\n",
    "\n",
    "    daily = df.resample(RESAMPLE_RULE).mean(numeric_only=True)\n",
    "\n",
    "    # garantir colunas requeridas\n",
    "    needed = [FORECAST_PARAMETER] + EXOG_VARS\n",
    "    missing = [c for c in needed if c not in daily.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Colunas faltando ap√≥s resample: {missing}\\nDispon√≠veis: {list(daily.columns)[:12]} ...\")\n",
    "\n",
    "    daily = daily.ffill()\n",
    "    return daily\n",
    "\n",
    "def series_for_coord(coord: Tuple[float, float], df_all_stations: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
    "    df = df_all_stations.copy()\n",
    "    df[\"Distancia\"] = df.apply(lambda r: calculate_distance_from_point_to_station(r, coord), axis=1)\n",
    "    df_nearest = df[df[\"Distancia\"] < RADIUS_KM]\n",
    "    if df_nearest.empty:\n",
    "        df_nearest = df.sort_values(\"Distancia\").head(1)  # fallback: esta√ß√£o mais pr√≥xima\n",
    "    return build_daily_panel_from_stations(df_nearest)\n",
    "\n",
    "# =============================\n",
    "# Ex√≥genas com m√°scaras (sem vazamento)\n",
    "# =============================\n",
    "def _profiles_from_train(df_training: pd.DataFrame, exog_vars):\n",
    "    prof_mmdd = (\n",
    "        df_training.assign(__mm__=df_training.index.month, __dd__=df_training.index.day)\n",
    "                   .groupby([\"__mm__\", \"__dd__\"])[exog_vars]\n",
    "                   .median()\n",
    "    )\n",
    "    prof_doy = (\n",
    "        df_training.assign(__doy__=df_training.index.dayofyear)\n",
    "                   .groupby(\"__doy__\")[exog_vars]\n",
    "                   .median()\n",
    "    )\n",
    "    return prof_mmdd, prof_doy\n",
    "\n",
    "def _impute_with_profiles(s: pd.Series, mmdd_index, doy_index, prof_mmdd, prof_doy, train_col_median):\n",
    "    need = s.isna()\n",
    "    if need.any():\n",
    "        s.loc[need] = prof_mmdd[s.name].reindex(mmdd_index[need]).values\n",
    "    need = s.isna()\n",
    "    if need.any():\n",
    "        s.loc[need] = prof_doy[s.name].reindex(doy_index[need]).values\n",
    "    need = s.isna()\n",
    "    if need.any():\n",
    "        s.loc[need] = train_col_median\n",
    "    return s\n",
    "\n",
    "def prepare_exog_with_masks(df_training: pd.DataFrame,\n",
    "                            df_test: pd.DataFrame,\n",
    "                            exog_vars,\n",
    "                            fourier_period: int = FOURIER_PERIOD,\n",
    "                            fourier_k: int = FOURIER_K):\n",
    "    \"\"\"\n",
    "    Retorna X_train, X_test, feature_names com:\n",
    "      [ex√≥genas imputadas | m√°scaras *_MISSING | Fourier]\n",
    "    M√°scaras: 1 quando o valor ORIGINAL era NaN, 0 caso contr√°rio.\n",
    "    Imputa√ß√£o do TESTE usa apenas estat√≠sticas do TREINO (sem vazamento).\n",
    "    \"\"\"\n",
    "    # m√°scaras originais\n",
    "    train_masks = {f\"{col}_MISSING\": df_training[col].isna().astype(int) for col in exog_vars}\n",
    "    test_masks  = {f\"{col}_MISSING\": df_test[col].isna().astype(int)      for col in exog_vars}\n",
    "\n",
    "    prof_mmdd, prof_doy = _profiles_from_train(df_training, exog_vars)\n",
    "\n",
    "    # √≠ndices auxiliares\n",
    "    mmdd_tr = pd.MultiIndex.from_arrays([df_training.index.month, df_training.index.day], names=[\"__mm__\", \"__dd__\"])\n",
    "    mmdd_te = pd.MultiIndex.from_arrays([df_test.index.month,     df_test.index.day    ], names=[\"__mm__\", \"__dd__\"])\n",
    "    doy_tr  = ((df_training.index.dayofyear - 1) % 365) + 1\n",
    "    doy_te  = ((df_test.index.dayofyear     - 1) % 365) + 1\n",
    "\n",
    "    tr_imp = df_training[exog_vars].copy()\n",
    "    te_imp = df_test[exog_vars].copy()\n",
    "\n",
    "    # imputa√ß√£o treino\n",
    "    for col in exog_vars:\n",
    "        tr_imp[col] = _impute_with_profiles(\n",
    "            tr_imp[col], mmdd_tr, doy_tr, prof_mmdd, prof_doy,\n",
    "            train_col_median=df_training[col].median(skipna=True)\n",
    "        )\n",
    "    # imputa√ß√£o teste (sem vazamento)\n",
    "    for col in exog_vars:\n",
    "        te_imp[col] = _impute_with_profiles(\n",
    "            te_imp[col], mmdd_te, doy_te, prof_mmdd, prof_doy,\n",
    "            train_col_median=df_training[col].median(skipna=True)\n",
    "        )\n",
    "\n",
    "    # Fourier\n",
    "    n_tr, n_te = len(df_training), len(df_test)\n",
    "    t_tr = np.arange(n_tr)\n",
    "    t_te = np.arange(n_tr, n_tr + n_te)\n",
    "    F_tr = create_fourier_terms(t_tr, fourier_period, fourier_k)\n",
    "    F_te = create_fourier_terms(t_te, fourier_period, fourier_k)\n",
    "    fourier_cols = [f\"F_{k}_{fn}\" for k in range(1, fourier_k + 1) for fn in (\"sin\", \"cos\")]\n",
    "\n",
    "    # monta X\n",
    "    X_train = np.hstack([\n",
    "        tr_imp.to_numpy(),\n",
    "        np.column_stack([train_masks[m].to_numpy() for m in train_masks]),\n",
    "        F_tr\n",
    "    ])\n",
    "    X_test  = np.hstack([\n",
    "        te_imp.to_numpy(),\n",
    "        np.column_stack([test_masks[m].to_numpy() for m in test_masks]),\n",
    "        F_te\n",
    "    ])\n",
    "\n",
    "    feature_names = list(exog_vars) + list(train_masks.keys()) + fourier_cols\n",
    "    return X_train, X_test, feature_names\n",
    "\n",
    "# =============================\n",
    "# ARIMA(3,0,1) com ex√≥genas + Fourier + calibra√ß√£o\n",
    "# =============================\n",
    "def debias_with_calibration(model_fit, df_training, forecast_parameter, X_train_final, forecast_vals, calib_days=60):\n",
    "    start = max(0, len(df_training) - calib_days)\n",
    "    pred_cal = model_fit.get_prediction(\n",
    "        start=df_training.index[start],\n",
    "        end=df_training.index[-1],\n",
    "        exog=X_train_final[start:]\n",
    "    ).predicted_mean\n",
    "    y_cal = df_training[forecast_parameter].iloc[start:]\n",
    "    ME = (y_cal - pred_cal).mean()\n",
    "    forecast_bias_fixed = forecast_vals + ME\n",
    "    X = sm.add_constant(pred_cal.values)\n",
    "    a, b = sm.OLS(y_cal.values, X).fit().params\n",
    "    forecast_linear_cal = a + b * forecast_vals\n",
    "    return {\"ME\": ME, \"forecast_bias_fixed\": forecast_bias_fixed, \"a\": a, \"b\": b, \"forecast_linear_cal\": forecast_linear_cal}\n",
    "\n",
    "def arima_forecast_with_fourier_terms_exog(df_training: pd.DataFrame,\n",
    "                                           df_test: pd.DataFrame,\n",
    "                                           forecast_parameter: str,\n",
    "                                           trend: str = \"ct\",\n",
    "                                           calib_days: int = 60) -> pd.Series:\n",
    "    X_train, X_test, _ = prepare_exog_with_masks(df_training, df_test, EXOG_VARS, FOURIER_PERIOD, FOURIER_K)\n",
    "\n",
    "    model = ARIMA(df_training[forecast_parameter], exog=X_train, order=ARIMA_ORDER, trend=trend)\n",
    "    model_fit = model.fit()\n",
    "\n",
    "    forecast_vals = model_fit.forecast(steps=len(df_test), exog=X_test)\n",
    "    cal = debias_with_calibration(model_fit, df_training, forecast_parameter, X_train, forecast_vals, calib_days=calib_days)\n",
    "\n",
    "    forecast_corrected = pd.Series(cal[\"forecast_bias_fixed\"], index=df_test.index, name=\"forecast\")\n",
    "    return forecast_corrected\n",
    "\n",
    "# =============================\n",
    "# Pipeline principal\n",
    "# =============================\n",
    "def run_for_coords(coords: List[Tuple[float, float]]) -> Dict[Tuple[float, float], Dict]:\n",
    "    df_all = load_all_stations()\n",
    "    results: Dict[Tuple[float, float], Dict] = {}\n",
    "\n",
    "    for coord in coords:\n",
    "        daily = series_for_coord(coord, df_all)\n",
    "        if daily is None:\n",
    "            results[coord] = {\"ok\": False, \"reason\": \"Sem dados para essa coordenada.\"}\n",
    "            continue\n",
    "\n",
    "        split_idx = int(len(daily) * SPLIT_TRAIN)\n",
    "        df_training = daily.iloc[:split_idx].copy()\n",
    "        df_test     = daily.iloc[split_idx:].copy()\n",
    "\n",
    "        # sanity check\n",
    "        needed = [FORECAST_PARAMETER] + EXOG_VARS\n",
    "        if any(col not in df_training.columns for col in needed) or any(col not in df_test.columns for col in needed):\n",
    "            results[coord] = {\"ok\": False, \"reason\": \"Colunas necess√°rias ausentes (target/ex√≥genas).\"}\n",
    "            continue\n",
    "\n",
    "        forecast = arima_forecast_with_fourier_terms_exog(df_training, df_test, FORECAST_PARAMETER)\n",
    "\n",
    "        # m√©tricas (RMSE com debias; MSE; sMAPE)\n",
    "        mse = mean_squared_error(df_test[FORECAST_PARAMETER], forecast)\n",
    "        rmse = mse ** 0.5\n",
    "        smape_val = smape(df_test[FORECAST_PARAMETER], forecast)\n",
    "\n",
    "        results[coord] = {\n",
    "            \"ok\": True,\n",
    "            \"periodo\": f\"{daily.index.min().date()} ‚Üí {daily.index.max().date()}\",\n",
    "            \"n_total\": len(daily),\n",
    "            \"n_train\": len(df_training),\n",
    "            \"n_test\": len(df_test),\n",
    "            \"RMSE\": float(rmse),           # j√° com debias\n",
    "            \"MSE\": float(mse),\n",
    "            \"sMAPE\": float(smape_val),\n",
    "            \"forecast\": forecast,          # s√©rie prevista (apenas janela de teste)\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "# =============================\n",
    "# Rodar para TODAS as universidades e imprimir tabela\n",
    "# =============================\n",
    "# L√™ CSV das universidades\n",
    "try:\n",
    "    df_uni = pd.read_csv(UNI_CSV)  # tenta separador padr√£o\n",
    "except Exception:\n",
    "    df_uni = pd.read_csv(UNI_CSV, sep=\";\", decimal=\",\")  # fallback\n",
    "\n",
    "df_uni.columns = [c.strip() for c in df_uni.columns]\n",
    "assert {\"Universidade\", \"Estado\", \"Latitude\", \"Longitude\"}.issubset(df_uni.columns), \\\n",
    "    f\"CSV de universidades n√£o tem as colunas esperadas. Tenho: {df_uni.columns.tolist()}\"\n",
    "\n",
    "# Monta coords na mesma ordem do CSV\n",
    "coords = [(float(lat), float(lon)) for lat, lon in zip(df_uni[\"Latitude\"], df_uni[\"Longitude\"])]\n",
    "\n",
    "# Executa\n",
    "results = run_for_coords(coords)\n",
    "\n",
    "# Tabela final\n",
    "rows = []\n",
    "for i, coord in enumerate(coords):\n",
    "    uni = df_uni.iloc[i][\"Universidade\"]\n",
    "    uf  = df_uni.iloc[i][\"Estado\"]\n",
    "    r   = results.get(coord, {})\n",
    "    if r.get(\"ok\"):\n",
    "        rows.append({\n",
    "            \"Universidade\": uni,\n",
    "            \"Estado\": uf,\n",
    "            \"Latitude\": round(coord[0], 6),\n",
    "            \"Longitude\": round(coord[1], 6),\n",
    "            \"Per√≠odo\": r[\"periodo\"],\n",
    "            \"Obs Totais\": r[\"n_total\"],\n",
    "            \"Treino\": r[\"n_train\"],\n",
    "            \"Teste\": r[\"n_test\"],\n",
    "            \"RMSE (debias)\": round(r[\"RMSE\"], 3),\n",
    "            \"MSE\": round(r[\"MSE\"], 3),\n",
    "            \"sMAPE (%)\": round(r[\"sMAPE\"], 3),\n",
    "        })\n",
    "    else:\n",
    "        rows.append({\n",
    "            \"Universidade\": uni,\n",
    "            \"Estado\": uf,\n",
    "            \"Latitude\": round(coord[0], 6),\n",
    "            \"Longitude\": round(coord[1], 6),\n",
    "            \"Status\": \"Falhou\",\n",
    "            \"Motivo\": r.get(\"reason\", \"Sem dados ap√≥s filtro/transforma√ß√µes\"),\n",
    "        })\n",
    "\n",
    "df_resultados = pd.DataFrame(rows)\n",
    "sort_cols = [c for c in [\"Status\", \"Estado\", \"Universidade\"] if c in df_resultados.columns]\n",
    "df_resultados = df_resultados.sort_values(by=sort_cols).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== RESULTADOS POR UNIVERSIDADE (ARIMA(3,0,1) + EX√ìGENAS + FOURIER + MASKS) ===\")\n",
    "print(df_resultados)\n",
    "\n",
    "# opcional: salvar\n",
    "# df_resultados.to_csv(\"resultados_arima_universidades.csv\", index=False, sep=\";\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
